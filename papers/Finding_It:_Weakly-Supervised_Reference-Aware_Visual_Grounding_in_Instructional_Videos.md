[**Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos**](https://github.com/Big-Brother-Pikachu/Paper-Contributions-Analysis#4-finding-it-weakly-supervised-reference-aware-visual-grounding-in-instructional-videos)

# Contributions in my words

1. They proposed a visually grounded action graph to explicitly capture the latent dependencies between grounding and reference in instructional video.
2. They proposed a joint framework for reference-aware visual grounding to effectively infer this graph from input video and transcription.
3. As the dense annotations are hard to acquire, they proposed a new reference-aware multiple instance learning (RA-MIL)
method for weakly-supervised learning.

# Contributions they claim

1. To formulate the visual grounding task for the video domain.
2. To present a novel visual grounding model that is both reference-aware and weakly-supervised.
3. To provide reference-grounding test set annotations for two main instructional video benchmarks.

# Contributions in my words modified